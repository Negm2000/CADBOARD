import tkinter as tk
from tkinter import filedialog, messagebox, ttk, simpledialog
from PIL import Image, ImageTk
import cv2
import ezdxf
import math
import numpy as np
import os
import traceback
import json
from scipy.spatial import KDTree
import time # Added for performance timing

# --- Conditional Import for IDS Camera ---
# Check for the ueye library and set a flag. This allows the application to run
# even if the specific IDS camera drivers/SDK are not installed.
try:
    from pyueye import ueye
    IS_IDS_CAMERA_AVAILABLE = True
except ImportError:
    print("WARNING: pyueye library not found. IDS Camera functionality will be disabled.")
    IS_IDS_CAMERA_AVAILABLE = False
    # If the library isn't found, create a mock class. This prevents crashes
    # when camera-related functions are called by returning default/error values.
    class MockUeye:
        def __getattr__(self, name):
            def method(*args, **kwargs):
                # Mock is_InitCamera to return an error code
                if name == 'is_InitCamera': return -1
                return 0 # Return a neutral value for other functions
            return method
    ueye = MockUeye()


class InspectionApp:
    """
    Main application class for the DIE-VIS Visualizer & Inspector.
    This class encapsulates the GUI, state management, and all image processing
    and computer vision logic for comparing a physical part (via image) to a
    CAD drawing (via DXF).
    """

    def __init__(self, root):
        """
        Initializes the main application window, UI elements, and state variables.
        :param root: The root Tkinter window.
        """
        self.root = root
        self.root.title("DIE-VIS: Visualizer & Inspector (Hybrid Geometric Engine)")
        self.root.geometry("1300x950")

        # --- State Variables ---
        # These variables hold the application's state, such as file paths,
        # loaded images, CAD data, and transformation matrices.
        self.image_path = None
        self.dxf_path = None
        self.cv_image = None             # The image currently being displayed (can be modified)
        self.original_cv_image = None    # The pristine image as loaded/captured
        self.cad_features = {'outline': np.array([]), 'holes': [], 'creases': []}
        self.last_transform_matrix = None # Stores the result of the last alignment
        self.last_transform_type = None  # 'affine' or 'homography'
        self.settings_file = "settings.json"
        self.last_ip_url = "http://"     # State for the last used IP camera URL

        # This mask is generated by the MOG2 background subtraction method
        # and used for more precise contour detection if available.
        self.precomputed_mask = None

        # --- UI Colors and Styles ---
        self.colors = {
            "bg": "#2E2E2E", "fg": "#FFFFFF", "btn": "#4A4A4A",
            "btn_active": "#5A5A5A", "accent": "#00AACC", "accent_fg": "#FFFFFF",
            "pass_fill": (0, 200, 0, 80), "fail_fill": (220, 30, 30, 90)
        }
        self.root.configure(bg=self.colors["bg"])
        self.setup_styles()

        # --- UI Layout ---
        # The UI is structured using frames for better organization.
        main_frame = tk.Frame(root, bg=self.colors["bg"])
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        top_controls_frame = tk.Frame(main_frame, bg=self.colors["bg"])
        top_controls_frame.pack(fill=tk.X, side=tk.TOP, pady=(0, 10))

        # Separate frames for each row of controls
        control_frame_1 = tk.Frame(top_controls_frame, bg=self.colors["bg"])
        control_frame_1.pack(fill=tk.X, pady=(0, 5))
        control_frame_2 = tk.Frame(top_controls_frame, bg=self.colors["bg"])
        control_frame_2.pack(fill=tk.X, pady=(0, 5))
        control_frame_3 = tk.Frame(top_controls_frame, bg=self.colors["bg"])
        control_frame_3.pack(fill=tk.X, pady=(0, 5))
        crease_controls_container = tk.Frame(top_controls_frame, bg=self.colors["bg"])
        crease_controls_container.pack(fill=tk.X, pady=(5,0))
        control_frame_4 = tk.Frame(top_controls_frame, bg=self.colors["bg"])
        control_frame_4.pack(fill=tk.X, pady=(5,10))

        self.image_frame = tk.Frame(main_frame, bg="#000000", relief=tk.SUNKEN, borderwidth=2)
        self.image_frame.pack(fill=tk.BOTH, expand=True)

        # --- Populate Control Widgets ---
        # Frame 1: Main action buttons for loading files and running inspections.
        self.btn_load_image = ttk.Button(control_frame_1, text="1a. Load Image", command=self.load_image)
        self.btn_load_image.pack(side=tk.LEFT, padx=5)
        self.btn_capture_ids = ttk.Button(control_frame_1, text="1b. Capture IDS", command=self.capture_from_ids)
        self.btn_capture_ids.pack(side=tk.LEFT, padx=5)
        self.btn_capture_mog2_ids = ttk.Button(control_frame_1, text="1c. MOG2 (IDS)", command=self.capture_with_mog2_from_ids)
        self.btn_capture_mog2_ids.pack(side=tk.LEFT, padx=5)
        self.btn_capture_phone = ttk.Button(control_frame_1, text="1d. MOG2 (Phone)", command=self.capture_from_phone_stream)
        self.btn_capture_phone.pack(side=tk.LEFT, padx=5)

        # Disable IDS camera buttons if the library is not available
        if not IS_IDS_CAMERA_AVAILABLE:
            self.btn_capture_ids.config(state=tk.DISABLED)
            self.btn_capture_mog2_ids.config(state=tk.DISABLED)

        self.btn_load_dxf = ttk.Button(control_frame_1, text="2. Load DXF", command=self.load_dxf)
        self.btn_load_dxf.pack(side=tk.LEFT, padx=(15,5))
        self.btn_visualize = ttk.Button(control_frame_1, text="Visualize", state=tk.DISABLED, command=self.run_visualization)
        self.btn_visualize.pack(side=tk.LEFT, padx=(20, 5))
        self.btn_inspect = ttk.Button(control_frame_1, text="Align (Affine)", state=tk.DISABLED, command=self.run_alignment_and_inspection)
        self.btn_inspect.pack(side=tk.LEFT, padx=5)
        self.btn_inspect_homography = ttk.Button(control_frame_1, text="Align (Homography)", state=tk.DISABLED, command=self.run_homography_inspection)
        self.btn_inspect_homography.pack(side=tk.LEFT, padx=5)
        self.btn_find_anomalies = ttk.Button(control_frame_1, text="Find Anomalies", state=tk.DISABLED, command=self.run_feature_specific_anomaly_detection, style="Accent.TButton")
        self.btn_find_anomalies.pack(side=tk.LEFT, padx=(15, 5))
        
        # Debug mode checkbox to enable/disable verbose logging and display of intermediate images
        self.debug_mode = tk.BooleanVar()
        self.chk_debug = tk.Checkbutton(control_frame_1, text="Debug", variable=self.debug_mode,
                                        bg=self.colors["bg"], fg=self.colors["fg"],
                                        selectcolor=self.colors["btn"], activebackground=self.colors["bg"],
                                        activeforeground=self.colors["fg"], highlightthickness=0)
        self.chk_debug.pack(side=tk.RIGHT, padx=(0, 5))
        
        # Frame 2: Sliders for setting Hole and Missing Material tolerances.
        self.lbl_hole_tolerance = tk.Label(control_frame_2, text="Hole Occlusion Tol (%):", bg=self.colors["bg"], fg=self.colors["fg"])
        self.lbl_hole_tolerance.pack(side=tk.LEFT, padx=(15, 5))
        self.hole_occlusion_tolerance_var = tk.DoubleVar(value=10.0)
        self.hole_tolerance_slider = ttk.Scale(control_frame_2, from_=1.0, to=99.0, orient=tk.HORIZONTAL, length=150, variable=self.hole_occlusion_tolerance_var, command=self._update_slider_labels)
        self.hole_tolerance_slider.pack(side=tk.LEFT, padx=5)
        self.lbl_hole_occlusion_val = tk.Label(control_frame_2, text="", bg=self.colors["bg"], fg=self.colors["accent"], width=6, anchor='w')
        self.lbl_hole_occlusion_val.pack(side=tk.LEFT, padx=(0, 20))

        self.lbl_missing_tolerance = tk.Label(control_frame_2, text="Missing Mat. Tol (% Diag):", bg=self.colors["bg"], fg=self.colors["fg"])
        self.lbl_missing_tolerance.pack(side=tk.LEFT, padx=(15, 5))
        self.missing_material_tolerance_var = tk.DoubleVar(value=0.5)
        self.missing_tolerance_slider = ttk.Scale(control_frame_2, from_=0.1, to=5.0, orient=tk.HORIZONTAL, length=150, variable=self.missing_material_tolerance_var, command=self._update_slider_labels)
        self.missing_tolerance_slider.pack(side=tk.LEFT, padx=5)
        self.lbl_missing_tol_val = tk.Label(control_frame_2, text="", bg=self.colors["bg"], fg=self.colors["accent"], width=6, anchor='w')
        self.lbl_missing_tol_val.pack(side=tk.LEFT, padx=5)

        # Frame 3: Slider for setting Extra Material tolerance.
        self.lbl_extra_tolerance = tk.Label(control_frame_3, text="Extra Mat. Tol (% Diag):  ", bg=self.colors["bg"], fg=self.colors["fg"])
        self.lbl_extra_tolerance.pack(side=tk.LEFT, padx=(15, 5))
        self.extra_material_tolerance_var = tk.DoubleVar(value=0.5)
        self.extra_tolerance_slider = ttk.Scale(control_frame_3, from_=0.1, to=5.0, orient=tk.HORIZONTAL, length=150, variable=self.extra_material_tolerance_var, command=self._update_slider_labels)
        self.extra_tolerance_slider.pack(side=tk.LEFT, padx=5)
        self.lbl_extra_tol_val = tk.Label(control_frame_3, text="", bg=self.colors["bg"], fg=self.colors["accent"], width=6, anchor='w')
        self.lbl_extra_tol_val.pack(side=tk.LEFT, padx=(0, 20))

        # Crease Controls: Parameters for the adaptive threshold method used for crease detection.
        self.lbl_block_size = tk.Label(crease_controls_container, text="Adaptive Block Size:", bg=self.colors["bg"], fg=self.colors["fg"])
        self.lbl_block_size.pack(side=tk.LEFT, padx=(15,5))
        self.block_size_var = tk.IntVar(value=25)
        self.block_size_slider = ttk.Scale(crease_controls_container, from_=3, to=101, orient=tk.HORIZONTAL, length=120, variable=self.block_size_var, command=self._update_slider_labels)
        self.block_size_slider.pack(side=tk.LEFT, padx=5)
        self.lbl_block_size_val = tk.Label(crease_controls_container, text="", bg=self.colors["bg"], fg=self.colors["accent"], width=4, anchor='w')
        self.lbl_block_size_val.pack(side=tk.LEFT, padx=(0,10))
        
        self.lbl_c_constant = tk.Label(crease_controls_container, text="Sensitivity (C):", bg=self.colors["bg"], fg=self.colors["fg"])
        self.lbl_c_constant.pack(side=tk.LEFT, padx=(10,5))
        self.c_constant_var = tk.IntVar(value=7)
        self.c_constant_slider = ttk.Scale(crease_controls_container, from_=1, to=30, orient=tk.HORIZONTAL, length=120, variable=self.c_constant_var, command=self._update_slider_labels)
        self.c_constant_slider.pack(side=tk.LEFT, padx=5)
        self.lbl_c_constant_val = tk.Label(crease_controls_container, text="", bg=self.colors["bg"], fg=self.colors["accent"], width=4, anchor='w')
        self.lbl_c_constant_val.pack(side=tk.LEFT, padx=(0,10))

        self.lbl_crease_fill = tk.Label(crease_controls_container, text="Min Crease Fill (%):", bg=self.colors["bg"], fg=self.colors["fg"])
        self.lbl_crease_fill.pack(side=tk.LEFT, padx=(10,5))
        self.min_crease_fill_var = tk.DoubleVar(value=15.0)
        self.min_crease_fill_slider = ttk.Scale(crease_controls_container, from_=1, to=100, orient=tk.HORIZONTAL, length=120, variable=self.min_crease_fill_var, command=self._update_slider_labels)
        self.min_crease_fill_slider.pack(side=tk.LEFT, padx=5)
        self.lbl_crease_fill_val = tk.Label(crease_controls_container, text="", bg=self.colors["bg"], fg=self.colors["accent"], width=6, anchor='w')
        self.lbl_crease_fill_val.pack(side=tk.LEFT, padx=0)

        # Frame 4: Status labels to show loaded file names.
        self.lbl_image_status = tk.Label(control_frame_4, text="Image: None", bg=self.colors["bg"], fg=self.colors["fg"], padx=10)
        self.lbl_image_status.pack(side=tk.LEFT)
        self.lbl_dxf_status = tk.Label(control_frame_4, text="DXF: None", bg=self.colors["bg"], fg=self.colors["fg"], padx=10)
        self.lbl_dxf_status.pack(side=tk.LEFT)

        # Image Display Setup
        self.image_label = tk.Label(self.image_frame, bg="#000000")
        self.image_label.pack(fill=tk.BOTH, expand=True)
        self.image_label.bind("<Configure>", self.on_resize) # Bind resize event

        # Load saved settings on startup and handle application closing
        self.load_settings()
        self.root.protocol("WM_DELETE_WINDOW", self._on_closing)

    def _on_closing(self):
        """Saves settings and cleanly closes the application."""
        self.save_settings()
        cv2.destroyAllWindows() # Ensure all OpenCV windows are closed
        self.root.destroy()

    def save_settings(self):
        """Saves the current slider and checkbox values to a JSON file."""
        settings = {
            "hole_occlusion_tolerance": self.hole_occlusion_tolerance_var.get(),
            "missing_material_tolerance": self.missing_material_tolerance_var.get(),
            "extra_material_tolerance": self.extra_material_tolerance_var.get(),
            "block_size": self.block_size_var.get(),
            "c_constant": self.c_constant_var.get(),
            "min_crease_fill": self.min_crease_fill_var.get(),
            "debug_mode": self.debug_mode.get(),
            "last_ip_url": self.last_ip_url, # Save the last used IP
        }
        try:
            with open(self.settings_file, 'w') as f:
                json.dump(settings, f, indent=4)
        except Exception as e:
            print(f"Error saving settings: {e}")

    def load_settings(self):
        """Loads settings from the JSON file, if it exists."""
        try:
            if os.path.exists(self.settings_file):
                with open(self.settings_file, 'r') as f:
                    settings = json.load(f)
                    self.hole_occlusion_tolerance_var.set(settings.get("hole_occlusion_tolerance", 10.0))
                    self.missing_material_tolerance_var.set(settings.get("missing_material_tolerance", 0.5))
                    self.extra_material_tolerance_var.set(settings.get("extra_material_tolerance", 0.5))
                    self.block_size_var.set(settings.get("block_size", 25))
                    self.c_constant_var.set(settings.get("c_constant", 7))
                    self.min_crease_fill_var.set(settings.get("min_crease_fill", 15.0))
                    self.debug_mode.set(settings.get("debug_mode", False))
                    self.last_ip_url = settings.get("last_ip_url", "http://") # Load the last used IP
        except Exception as e:
            print(f"Error loading settings, using defaults. Error: {e}")
        finally:
            self._update_slider_labels() # Ensure labels reflect loaded values

    def _update_slider_labels(self, *args):
        """Updates the text labels next to the sliders with their current values."""
        # Ensure block size is always an odd number, as required by adaptiveThreshold
        block_val = self.block_size_var.get()
        if block_val % 2 == 0:
            self.block_size_var.set(block_val + 1)
        self.lbl_hole_occlusion_val.config(text=f"{self.hole_occlusion_tolerance_var.get():.1f}%")
        self.lbl_missing_tol_val.config(text=f"{self.missing_material_tolerance_var.get():.1f}%")
        self.lbl_extra_tol_val.config(text=f"{self.extra_material_tolerance_var.get():.1f}%")
        self.lbl_block_size_val.config(text=f"{self.block_size_var.get()}")
        self.lbl_c_constant_val.config(text=f"{self.c_constant_var.get()}")
        self.lbl_crease_fill_val.config(text=f"{self.min_crease_fill_var.get():.1f}%")
            
    def setup_styles(self):
        """Configures the visual styles for ttk widgets for a modern look."""
        style = ttk.Style()
        style.theme_use('clam')
        style.configure("TButton", padding=6, relief="flat", background=self.colors["btn"], foreground=self.colors["fg"], font=('Helvetica', 10, 'bold'), borderwidth=0)
        style.map("TButton", background=[('active', self.colors["btn_active"]), ('disabled', '#3D3D3D')], foreground=[('disabled', '#888888')])
        style.configure("Accent.TButton", background=self.colors["accent"], foreground=self.colors["accent_fg"], font=('Helvetica', 10, 'bold'))
        style.map("Accent.TButton", background=[('active', '#005f9e'), ('disabled', '#3D3D3D')])
        style.configure("TFrame", background=self.colors["bg"])
        style.configure("TLabel", background=self.colors["bg"], foreground=self.colors["fg"], font=('Helvetica', 10))
        style.configure("Horizontal.TScale", background=self.colors["bg"], troughcolor=self.colors["btn"])
        style.configure("TCombobox", fieldbackground=self.colors["btn"], background=self.colors["btn"], foreground=self.colors["fg"], arrowcolor=self.colors["fg"], bordercolor=self.colors["bg"])
        style.map('TCombobox', fieldbackground=[('readonly', self.colors["btn"])], selectbackground=[('readonly', self.colors["btn"])], selectforeground=[('readonly', self.colors["fg"])])

    # --- Feature Detection ---
    def find_image_contour(self, image):
        """
        Finds the main external contour of the object in the image.
        It prioritizes using the precomputed MOG2 mask if available, as it is
        generally more accurate. Otherwise, it falls back to an HSV color-based method.
        """
        if self.precomputed_mask is not None:
            print("Finding contour using precomputed MOG2 mask...")
            contours, _ = cv2.findContours(self.precomputed_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if not contours: return None
            largest_contour = max(contours, key=cv2.contourArea)
            return largest_contour.astype(np.float32)

        print("Finding contour using 'HSV Color' method...")
        return self._find_contour_hsv(image)

    def find_image_contours_with_holes(self, image):
        """
        Finds all contours, including internal holes.
        Prioritizes the precomputed MOG2 mask if available.
        """
        if self.precomputed_mask is not None:
            print("Finding contours with holes using precomputed MOG2 mask...")
            # Use RETR_CCOMP to get a two-level hierarchy (external and internal contours)
            contours, hierarchy = cv2.findContours(self.precomputed_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)
            return contours, hierarchy

        print("Finding contours with holes using 'HSV Color' method...")
        return self._find_contours_with_holes_hsv(image)

    def _find_contour_hsv(self, image):
        """Helper function to get only the largest contour using the HSV method."""
        contours, _ = self._find_contours_with_holes_hsv(image)
        if not contours: return None
        return max(contours, key=cv2.contourArea).astype(np.float32)

    def _find_contours_with_holes_hsv(self, image):
        """Detects object contours based on a hardcoded HSV color range (for brassy/golden colors)."""
        if image is None: return None, None
        hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        # These bounds are tuned for a specific material color.
        lower_bound_hsv, upper_bound_hsv = np.array([5, 50, 50]), np.array([50, 255, 255])
        color_mask = cv2.inRange(hsv_image, lower_bound_hsv, upper_bound_hsv)

        if self.debug_mode.get():
            self._resize_for_display("DEBUG: HSV Mask (Outline)", color_mask)

        # Use RETR_CCOMP to get holes.
        return cv2.findContours(color_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)

    # --- Main Application Logic ---
    def run_feature_specific_anomaly_detection(self):
        """
        The core inspection function. It performs a detailed analysis of the aligned
        part, checking for extra material, missing material, hole occlusions, and crease defects.
        """
        print("\n--- Starting Full Anomaly Detection ---")
        t_start_anomaly_check = time.time() # Start timer for the entire function
        
        if self.last_transform_matrix is None or self.original_cv_image is None:
            messagebox.showerror("Prerequisite Error", "Image must be loaded and aligned first.")
            return

        h, w = self.original_cv_image.shape[:2]
        # Choose the correct transformation function based on the last alignment method
        transform_func = cv2.transform if self.last_transform_type == 'affine' else cv2.perspectiveTransform
        visualization_img = self.original_cv_image.copy()
        
        # --- Anomaly Counters ---
        extra_material_anomalies = 0
        missing_material_anomalies = 0
        hole_anomalies = 0
        crease_defects = 0
        
        # --- Mask Generation ---
        overlay = np.zeros((h, w, 4), dtype=np.uint8) # For drawing semi-transparent results
        raw_image_mask = np.zeros((h, w), dtype=np.uint8) # Mask of the detected physical part

        # 1. Get all contours from the physical part.
        t_start_contour_holes = time.time()
        image_contours_raw, _ = self.find_image_contours_with_holes(self.original_cv_image)
        if self.debug_mode.get():
            elapsed_contour_holes = time.time() - t_start_contour_holes
            print(f"[DEBUG] T_ContourDetect (w/Holes): {elapsed_contour_holes:.4f} seconds")

        if not image_contours_raw:
            messagebox.showerror("Detection Error", "Could not find any object contours in the image."); return
        # Create a filled mask of the detected object
        cv2.drawContours(raw_image_mask, image_contours_raw, -1, 255, cv2.FILLED)

        # 2. Align CAD features to the image space.
        aligned_cad_outline = transform_func(self.cad_features['outline'].reshape(-1, 1, 2), self.last_transform_matrix)
        aligned_cad_holes = [transform_func(h.reshape(-1, 1, 2), self.last_transform_matrix) for h in self.cad_features['holes']]
        aligned_cad_outline_int = aligned_cad_outline.astype(np.int32)
        
        # 3. Create a mask of the ideal CAD outline.
        outline_area_mask = np.zeros((h, w), dtype=np.uint8)
        cv2.drawContours(outline_area_mask, [aligned_cad_outline_int], -1, 255, -1)

        # --- Anomaly Checks ---
        print(" - Checking for outline, hole, and material defects...")
        
        # Clean the physical mask by removing anything outside the ideal CAD outline area.
        cleaned_physical_mask = cv2.bitwise_and(raw_image_mask, outline_area_mask)
        object_diagonal = math.sqrt(w**2 + h**2)

        # 4. Check for EXTRA material.
        extra_tolerance_px = max(1, int(round(object_diagonal * (self.extra_material_tolerance_var.get() / 100.0))))
        kernel_dilate = np.ones((extra_tolerance_px, extra_tolerance_px), np.uint8)
        upper_bound_mask = cv2.dilate(outline_area_mask, kernel_dilate) # Expand the ideal shape by the tolerance
        extra_material_mask = cv2.subtract(cleaned_physical_mask, upper_bound_mask) # Find pixels in physical part outside the tolerance
        contours, _ = cv2.findContours(extra_material_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            for contour in contours:
                if cv2.contourArea(contour) < 4: continue # Ignore tiny noise
                extra_material_anomalies += 1
                cv2.drawContours(visualization_img, [contour], -1, (0, 165, 255), -1)
                M = cv2.moments(contour)
                if M["m00"] != 0:
                    cx = int(M["m10"] / M["m00"]); cy = int(M["m01"] / M["m00"])
                    cv2.putText(visualization_img, "Extra", (cx - 20, cy + 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

        # 5. Check for MISSING material and HOLE defects.
        missing_tolerance_px = max(1, int(round(object_diagonal * (self.missing_material_tolerance_var.get() / 100.0))))
        kernel_erode = np.ones((missing_tolerance_px, missing_tolerance_px), np.uint8)
        lower_bound_mask = cv2.erode(outline_area_mask, kernel_erode) # Shrink the ideal shape by the tolerance
        missing_material_mask = cv2.subtract(lower_bound_mask, cleaned_physical_mask) # Find pixels in tolerance area that are empty
        all_missing_contours, _ = cv2.findContours(missing_material_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)
        
        if all_missing_contours:
            processed_contour_indices = set() # Keep track of contours that are part of expected holes

            # 5a. Check HOLE occlusion.
            for expected_hole in aligned_cad_holes:
                total_detected_area_in_hole = 0
                # Check which 'missing' contours are actually inside this expected hole
                for i, contour in enumerate(all_missing_contours):
                    M = cv2.moments(contour)
                    if M["m00"] == 0: continue
                    cx, cy = int(M["m10"] / M["m00"]), int(M["m01"] / M["m00"])
                    if cv2.pointPolygonTest(expected_hole, (float(cx), float(cy)), False) >= 0:
                        total_detected_area_in_hole += cv2.contourArea(contour)
                        processed_contour_indices.add(i) # Mark this contour as processed

                expected_hole_area = cv2.contourArea(expected_hole)
                presence_ratio = min(1.0, total_detected_area_in_hole / expected_hole_area if expected_hole_area > 0 else 0.0)
                occlusion_ratio = 1.0 - presence_ratio
                
                # If the hole is more occluded than the tolerance allows, flag it.
                if occlusion_ratio > (self.hole_occlusion_tolerance_var.get() / 100.0):
                    hole_anomalies += 1
                    M_hole = cv2.moments(expected_hole)
                    label_cx = int(M_hole["m10"] / M_hole["m00"]) if M_hole["m00"] > 0 else 0
                    label_cy = int(M_hole["m01"] / M_hole["m00"]) if M_hole["m00"] > 0 else 0
                    cv2.drawContours(visualization_img, [expected_hole.astype(np.int32)], -1, (255, 100, 0), -1)
                    label = f"Occluded ({occlusion_ratio*100:.0f}%)"
                    cv2.putText(visualization_img, label, (label_cx - 50, label_cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)
            
            # 5b. Check for other MISSING material.
            # Any 'missing' contours that weren't inside an expected hole are true defects.
            for i, contour in enumerate(all_missing_contours):
                if i in processed_contour_indices or cv2.contourArea(contour) < 5: continue
                missing_material_anomalies += 1
                cv2.drawContours(visualization_img, [contour], -1, (0, 0, 255), -1)
                M = cv2.moments(contour)
                if M["m00"] > 0:
                    cx, cy = int(M["m10"] / M["m00"]), int(M["m01"] / M["m00"])
                    cv2.putText(visualization_img, "Missing", (cx - 30, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)
        
        # 6. Check for CREASE defects.
        print(" - Checking for crease defects...")
        gray_img = cv2.cvtColor(self.original_cv_image, cv2.COLOR_BGR2GRAY)
        blurred_img = cv2.GaussianBlur(gray_img, (5, 5), 0)
        block_size = self.block_size_var.get()
        c_constant = self.c_constant_var.get()
        min_fill_percent = self.min_crease_fill_var.get()
        # Use adaptive thresholding to find dark lines (creases) in the image.
        adaptive_thresh_img = cv2.adaptiveThreshold(blurred_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, block_size, c_constant)

        if self.debug_mode.get():
            self._resize_for_display("DEBUG: Adaptive Threshold Result (Creases)", adaptive_thresh_img)

        final_labels = [] # To draw text labels last, preventing them from being overwritten.
        for crease in self.cad_features['creases']:
            if crease.size < 4: continue
            transformed_crease_float = transform_func(crease.reshape(-1, 1, 2), self.last_transform_matrix)
            transformed_crease_int = transformed_crease_float.astype(np.int32)
            
            # Create a "corridor" mask along the expected crease path.
            corridor_width = max(3, int(block_size / 3)) 
            corridor_mask = np.zeros((h, w), dtype=np.uint8)
            cv2.polylines(corridor_mask, [transformed_crease_int], False, 255, corridor_width)
            corridor_area = cv2.countNonZero(corridor_mask)
            if corridor_area == 0: continue
            
            # Check how much of the corridor is filled with detected pixels from the thresholded image.
            detected_pixels_in_corridor = cv2.bitwise_and(adaptive_thresh_img, corridor_mask)
            fill_factor = (cv2.countNonZero(detected_pixels_in_corridor) / corridor_area) * 100
            
            is_ok = fill_factor >= min_fill_percent
            fill_color = self.colors['pass_fill'] if is_ok else self.colors['fail_fill']
            cv2.polylines(overlay, [transformed_crease_int], False, fill_color, corridor_width)
            status = "OK"
            if not is_ok:
                crease_defects += 1
                status = "Defect"
            
            mid_point = tuple(transformed_crease_int[len(transformed_crease_int)//2][0])
            label_text = f"Fill: {fill_factor:.0f}% ({status})"
            final_labels.append((label_text, mid_point, corridor_width))
        
        # --- Final Visualization ---
        # Blend the semi-transparent overlay with the main image.
        alpha_mask = cv2.cvtColor(overlay[:, :, 3], cv2.COLOR_GRAY2BGR) / 255.0
        visualization_img = (visualization_img * (1 - alpha_mask) + overlay[:, :, :3] * alpha_mask).astype(np.uint8)
        # Redraw the CAD outline on top for clarity.
        cv2.polylines(visualization_img, [aligned_cad_outline_int], True, (0, 255, 255), 1)

        # Draw the text labels for creases.
        for text, pos, width in final_labels:
            (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)
            text_pos = (pos[0] - tw // 2, pos[1] - width)
            # Draw a black "outline" for the text to make it readable on any background.
            cv2.putText(visualization_img, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0,0,0), 4, cv2.LINE_AA)
            cv2.putText(visualization_img, text, text_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), 1, cv2.LINE_AA)

        total_anomalies = extra_material_anomalies + missing_material_anomalies + hole_anomalies + crease_defects
        
        # --- Create Summary and Display ---
        summary_message = (
            f"Found {total_anomalies} total potential anomalies.\n\n"
            f"--- Anomaly Summary ---\n"
            f"Extra Material Defects: {extra_material_anomalies}\n"
            f"Missing Material Defects: {missing_material_anomalies}\n"
            f"Occluded / Faulty Holes: {hole_anomalies} / {len(aligned_cad_holes)}\n"
            f"Crease Defects: {crease_defects} / {len(self.cad_features['creases'])}"
        )
        
        if self.debug_mode.get():
            elapsed_anomaly = time.time() - t_start_anomaly_check
            print(f"[DEBUG] T_AnomalyCheck: {elapsed_anomaly:.4f} seconds")
            print(f"[DEBUG] --- ANOMALY SUMMARY ---\n{summary_message.replace('--- Anomaly Summary ---\\n', '')}")

        self.update_image_display(visualization_img)
        messagebox.showinfo("Inspection Complete", summary_message)
        print(f"--- Anomaly Detection Complete: {total_anomalies} issues found. ---")
    
    def load_image(self):
        """Opens a file dialog to load an image from the disk."""
        filepath = filedialog.askopenfilename(title="Select an Image", filetypes=[("Image Files", "*.jpg *.jpeg *.png *.bmp"), ("All files", "*.*")])
        if not filepath: return
        self.image_path = filepath
        try:
            self.original_cv_image = cv2.imread(self.image_path)
            if self.original_cv_image is None: raise ValueError("OpenCV could not read the image file.")
            self.cv_image = self.original_cv_image.copy()
            self.lbl_image_status.config(text=f"Image: {os.path.basename(self.image_path)}")
            self.update_image_display(self.cv_image)
            self.precomputed_mask = None # Reset MOG2 mask when loading a new image
            self._check_files_loaded()
            self.last_transform_matrix = None # Reset alignment
        except Exception as e:
            messagebox.showerror("Image Load Error", f"Failed to load image: {e}")
            self.reset_image_state()

    def capture_from_ids(self):
        """Captures a single frame from a connected IDS (uEye) camera."""
        h_cam = ueye.HIDS(0) 
        mem_ptr = ueye.c_mem_p()
        mem_id = ueye.int()
        try:
            ret = ueye.is_InitCamera(h_cam, None)
            if ret != ueye.IS_SUCCESS:
                messagebox.showerror("IDS Camera Error", f"Could not initialize camera. Error code: {ret}")
                return
            
            # Get camera sensor info to correctly size the image buffer
            sensor_info = ueye.SENSORINFO()
            ueye.is_GetSensorInfo(h_cam, sensor_info)
            width, height = int(sensor_info.nMaxWidth), int(sensor_info.nMaxHeight)
            bits_per_pixel = 24 # For BGR8 format

            # Configure camera and capture a frame
            ueye.is_SetColorMode(h_cam, ueye.IS_CM_BGR8_PACKED)
            ueye.is_AllocImageMem(h_cam, width, height, bits_per_pixel, mem_ptr, mem_id)
            ueye.is_SetImageMem(h_cam, mem_ptr, mem_id)
            ueye.is_FreezeVideo(h_cam, ueye.IS_WAIT) # Capture one frame
            
            # Copy data from camera buffer to a numpy array
            array = ueye.get_data(mem_ptr, width, height, bits_per_pixel, width * bits_per_pixel // 8, copy=True)

            # Reshape the 1D array into a 3-channel image
            self.original_cv_image = np.reshape(array, (height, width, 3))
            self.cv_image = self.original_cv_image.copy()
            self.lbl_image_status.config(text="Image: From IDS Camera")
            self.image_path = "IDS_Capture"
            self.update_image_display(self.cv_image)
            self.precomputed_mask = None
            self._check_files_loaded()
            self.last_transform_matrix = None 
            messagebox.showinfo("Success", "Image captured successfully from IDS camera.")
        except Exception as e:
            messagebox.showerror("IDS Camera Error", f"An unexpected error occurred: {traceback.format_exc()}")
        finally:
            # Clean up camera resources
            if mem_ptr: ueye.is_FreeImageMem(h_cam, mem_ptr, mem_id)
            if h_cam: ueye.is_ExitCamera(h_cam)

    def capture_from_phone_stream(self):
        """Initiates the MOG2 capture process using an IP camera stream (e.g., from a phone)."""
        # Ask for URL, pre-filling with the last known one for convenience.
        url = simpledialog.askstring("IP Camera Stream", 
                                     "Enter the full phone camera stream URL:\n(e.g., http://192.168.1.100:8080/video)", 
                                     parent=self.root,
                                     initialvalue=self.last_ip_url)
        if url:
            # The capture_with_mog2 function will handle saving the URL on success.
            self.capture_with_mog2(source=url)

    def capture_with_mog2(self, source=0):
        """
        Performs background subtraction (MOG2) to isolate a newly introduced object from the background.
        It first learns the background, then captures the object and creates a high-quality mask.
        :param source: The video source (0 for default webcam, or a URL string for an IP camera).
        """
        print(f"\n--- Initializing MOG2 Capture from source: {source} ---")
        cap = None
        try:
            cap = cv2.VideoCapture(source)
            if not cap.isOpened():
                raise IOError(f"Cannot open video source: {source}")
            
            # If using a URL, save it for next time.
            if isinstance(source, str):
                self.last_ip_url = source
                print(f"         Successfully connected to {source}. Saved as default for next time.")

        except Exception as e:
            messagebox.showerror("Video Source Error", f"Could not open source. Please check the URL/device.\nError: {e}")
            if cap: cap.release()
            return
        
        # Initialize the MOG2 background subtractor
        fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)
        LEARNING_FRAMES = 50 # Number of frames to learn the initial background
        frame_count = 0
        
        window_name = "MOG2 Capture - Position background, then introduce object. Press 'q' to finalize."
        
        capture_frame = None # Will store the final captured image
        final_mask = None # Will store the final generated mask
        
        while True:
            ret, frame = cap.read()
            if not ret: break
            
            frame_count += 1
            if source == 0: frame = cv2.flip(frame, 1) # Flip webcam feed for intuitive movement
            
            # During the learning phase, the learning rate is default (-1).
            # After, it's set to 0 to lock the background model.
            learning_rate = -1 if frame_count <= LEARNING_FRAMES else 0
            fgmask = fgbg.apply(frame, learningRate=learning_rate)
            # Create a binary mask of the foreground (shadows are gray, objects are white)
            _, binary_mask = cv2.threshold(fgmask, 254, 255, cv2.THRESH_BINARY)

            # Clean the mask to show only the largest object and its holes
            display_mask = self._create_hierarchical_mask(binary_mask)
            
            # Once learning is done, keep updating the frame to be captured
            if frame_count > LEARNING_FRAMES:
                capture_frame = frame.copy()

            # Provide visual feedback to the user
            status_text, color = (f"Status: Learning... ({frame_count}/{LEARNING_FRAMES})", (0, 0, 255)) if frame_count <= LEARNING_FRAMES else ("Status: Background Locked", (0, 255, 0))
            
            display_img = cv2.cvtColor(display_mask, cv2.COLOR_GRAY2BGR)
            cv2.putText(display_img, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            combined_view = np.hstack((frame, display_img))
            
            # Resize window if it's too large for the screen
            h_disp, w_disp = combined_view.shape[:2]
            if w_disp > 1600:
                new_w = 1600
                new_h = int((new_w / w_disp) * h_disp)
                combined_view = cv2.resize(combined_view, (new_w, new_h))
                
            cv2.imshow(window_name, combined_view)

            # User presses 'q' to finalize the capture
            if cv2.waitKey(30) & 0xFF == ord('q'):
                if frame_count > LEARNING_FRAMES:
                    print("\n--- Stabilizing mask... Hold still. ---")
                    # Accumulate a few more frames to get a very stable final mask
                    final_mask = self._accumulate_frames_for_mask(cap, fgbg, source)
                    break
                else:
                    print("Still learning background, please wait...")

        cap.release()
        cv2.destroyAllWindows()
        
        # If capture was successful, update the application state
        if capture_frame is not None and final_mask is not None:
            print("--- MOG2 Capture Complete ---")
            self.original_cv_image = capture_frame
            self.cv_image = capture_frame.copy()
            self.precomputed_mask = final_mask
            self.lbl_image_status.config(text="Image: From MOG2 Capture")
            self.image_path = "MOG2_Capture"
            self.update_image_display(self.cv_image)
            self._check_files_loaded()
            self.last_transform_matrix = None
        else:
            messagebox.showwarning("Capture Warning", "No frame was captured from MOG2 process.")

    def _create_hierarchical_mask(self, binary_mask, min_hole_area=50):
        """
        Helper function to clean a raw MOG2 mask. It finds the largest contour,
        fills it, and then "cuts out" any internal holes above a certain size.
        This removes noise and gives a clean object silhouette.
        """
        contours, hierarchy = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        clean_mask = np.zeros(binary_mask.shape, dtype=np.uint8)
        
        if contours:
            contour_areas = [cv2.contourArea(c) for c in contours]
            if any(ca > 0 for ca in contour_areas):
                # Find and draw the largest contour (the object outline)
                largest_contour_index = np.argmax(contour_areas)
                cv2.drawContours(clean_mask, contours, largest_contour_index, 255, thickness=cv2.FILLED)
                
                # Find contours that are children of the largest contour (holes)
                if hierarchy is not None:
                    for i, contour in enumerate(contours):
                        # The 4th value in hierarchy is the parent index
                        if hierarchy[0][i][3] == largest_contour_index and cv2.contourArea(contour) > min_hole_area:
                            cv2.drawContours(clean_mask, [contour], -1, 0, thickness=cv2.FILLED) # Draw hole in black
        return clean_mask

    def _accumulate_frames_for_mask(self, cap, fgbg, source):
        """
        Captures a burst of frames and creates a final, stable mask using a
        majority vote. This helps eliminate flicker or small movements.
        """
        ACCUMULATION_FRAMES = 20
        h, w = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        accumulator = np.zeros((h, w), dtype=np.float32)
        
        for i in range(ACCUMULATION_FRAMES):
            ret, frame = cap.read()
            if not ret: continue
            if source == 0: frame = cv2.flip(frame, 1)

            # Continue applying MOG2 with a locked background model
            fgmask = fgbg.apply(frame, learningRate=0)
            _, binary_mask = cv2.threshold(fgmask, 254, 255, cv2.THRESH_BINARY)
            
            clean_mask = self._create_hierarchical_mask(binary_mask)
            # Add the normalized (0-1) mask to the accumulator
            accumulator += clean_mask / 255.0 
            
            # Display progress to the user
            progress_img = cv2.cvtColor(clean_mask, cv2.COLOR_GRAY2BGR)
            cv2.putText(progress_img, f"Stabilizing... {i+1}/{ACCUMULATION_FRAMES}", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,255), 2)
            cv2.imshow("MOG2 Capture - Position background, then introduce object. Press 'q' to finalize.", np.hstack((frame, progress_img)))
            cv2.waitKey(1)

        # Majority vote: if a pixel was "on" in >90% of the stabilization frames, it's part of the final mask.
        final_mask = (accumulator > (ACCUMULATION_FRAMES * 0.9)).astype(np.uint8) * 255
        return final_mask


    def capture_with_mog2_from_ids(self):
        """
        Placeholder for implementing MOG2 background subtraction with an IDS camera.
        This would require a continuous video stream from the IDS camera, unlike the
        single-frame capture currently implemented.
        """
        print("\n--- Initializing MOG2 Capture from IDS Camera ---")
        messagebox.showinfo("Not Implemented", "Multi-frame accumulation for IDS cameras is not yet implemented in this version.")
        return 

    def load_dxf(self):
        """
        Opens a file dialog to load a DXF file. It parses the file, looking for specific
        layers ("OUTLINE", "HOLES", "CREASES") and assembles the geometric entities
        on those layers into continuous paths.
        """
        filepath = filedialog.askopenfilename(title="Select a DXF File", filetypes=[("DXF Files", "*.dxf"), ("All files", "*.*")])
        if not filepath: return
        self.dxf_path = filepath
        try:
            doc = ezdxf.readfile(self.dxf_path)
            msp = doc.modelspace()
            self.cad_features = {'outline': np.array([]), 'holes': [], 'creases': []}

            # Helper function to process all entities on a given layer
            def process_layer(layer_name):
                segments = []
                # Query for all entities on the specified layer
                for entity in msp.query(f'*[layer=="{layer_name}"]'):
                    points = self._extract_entity_points(entity)
                    # Convert paths into line segments
                    if len(points) > 1:
                        for i in range(len(points) - 1): segments.append((points[i], points[i+1]))
                # Reassemble the segments back into continuous paths
                assembled_paths = self._assemble_paths(segments)
                return [np.array(p, dtype=np.float32) for p in assembled_paths]

            outlines = process_layer("OUTLINE")
            holes = process_layer("HOLES")
            creases = process_layer("CREASES")
            
            # Assume the longest path on the OUTLINE layer is the main outline
            if outlines:
                self.cad_features['outline'] = max(outlines, key=lambda p: cv2.arcLength(p.reshape(-1, 1, 2), False))
            self.cad_features['holes'] = holes
            self.cad_features['creases'] = creases
            
            self.lbl_dxf_status.config(text=f"DXF: {os.path.basename(self.dxf_path)}")
            self._check_files_loaded()
            self.last_transform_matrix = None # Reset alignment
        except Exception as e:
            messagebox.showerror("DXF Load Error", f"An error occurred during DXF processing:\n\n{traceback.format_exc()}")
            self.reset_dxf_state()

    def run_alignment_and_inspection(self):
        """
        Runs the affine alignment pipeline (Moments + ICP) and visualizes the result
        by overlaying the aligned CAD features on the image.
        """
        print("\n--- Starting Alignment & Inspection (Affine/Moments+ICP Pipeline) ---")
        t_start_align = time.time()
        transform_matrix = self.align_with_moments_icp_pipeline()
        
        if self.debug_mode.get():
            elapsed_align = time.time() - t_start_align
            print(f"[DEBUG] T_Alignment (Affine): {elapsed_align:.4f} seconds")

        if transform_matrix is None:
            messagebox.showerror("Alignment Error", "Could not compute the transformation matrix.")
            self.last_transform_matrix = None
            return
        
        # Store the successful transformation for later use (e.g., by anomaly detection)
        self.last_transform_matrix = transform_matrix
        self.last_transform_type = 'affine'
        
        # Visualize the result
        result_image = self.original_cv_image.copy()
        transformed_outline = cv2.transform(self.cad_features['outline'].reshape(-1, 1, 2), transform_matrix)
        cv2.polylines(result_image, [transformed_outline.astype(np.int32)], True, (0, 255, 255), 2) # Outline in cyan
        for hole in self.cad_features['holes']:
            if hole.size > 0:
                transformed_hole = cv2.transform(hole.reshape(-1,1,2), transform_matrix)
                cv2.polylines(result_image, [transformed_hole.astype(np.int32)], True, (255, 0, 255), 2) # Holes in magenta
        for crease in self.cad_features['creases']:
            if crease.size > 0:
                transformed_crease = cv2.transform(crease.reshape(-1,1,2), transform_matrix)
                cv2.polylines(result_image, [transformed_crease.astype(np.int32)], False, (255, 255, 0), 2) # Creases in yellow
        
        self.update_image_display(result_image)
        messagebox.showinfo("Inspection Complete", "DXF features aligned using the Affine (Moments+ICP) pipeline.")

    def run_homography_inspection(self):
        """
        Runs the homography alignment pipeline, which can account for perspective distortion.
        It visualizes the result by overlaying the aligned CAD features.
        """
        print("\n--- Starting Smart Homography Alignment ---")
        t_start_homography = time.time()
        homography_matrix, method_used = self.align_with_homography_pipeline()

        if self.debug_mode.get():
            elapsed_homography = time.time() - t_start_homography
            print(f"[DEBUG] T_Alignment (Homography): {elapsed_homography:.4f} seconds")
        
        if homography_matrix is None:
            messagebox.showerror("Alignment Error", "Could not compute the homography matrix.")
            self.last_transform_matrix = None
            return

        self.last_transform_matrix = homography_matrix
        self.last_transform_type = 'homography'

        # Visualize the result
        result_image = self.original_cv_image.copy()
        transformed_outline = cv2.perspectiveTransform(self.cad_features['outline'].reshape(-1, 1, 2), homography_matrix)
        cv2.polylines(result_image, [transformed_outline.astype(np.int32)], True, (0, 255, 255), 2)
        for hole in self.cad_features['holes']:
            if hole.size > 0:
                transformed_hole = cv2.perspectiveTransform(hole.reshape(-1,1,2), homography_matrix)
                cv2.polylines(result_image, [transformed_hole.astype(np.int32)], True, (255, 0, 255), 2)
        for crease in self.cad_features['creases']:
            if crease.size > 0:
                transformed_crease = cv2.perspectiveTransform(crease.reshape(-1,1,2), homography_matrix)
                cv2.polylines(result_image, [transformed_crease.astype(np.int32)], False, (255, 255, 0), 2)

        self.update_image_display(result_image)
        messagebox.showinfo("Inspection Complete", f"DXF features aligned using the {method_used} method.")
        
    def _check_files_loaded(self):
        """
        Enables or disables the main action buttons based on whether both an
        image and a valid DXF file have been loaded.
        """
        all_loaded = self.image_path is not None and self.dxf_path is not None and self.cad_features['outline'].size > 0
        state = tk.NORMAL if all_loaded else tk.DISABLED
        self.btn_visualize.config(state=state)
        self.btn_inspect.config(state=state)
        self.btn_inspect_homography.config(state=state)
        self.btn_find_anomalies.config(state=state)

    def on_resize(self, event=None):
        """Event handler for when the main window is resized. Redraws the image to fit."""
        self.update_image_display(self.cv_image)

    def update_image_display(self, image_to_show):
        """
        Resizes an OpenCV image to fit within the display label while maintaining aspect ratio,
        converts it to a format Tkinter can use, and updates the label.
        """
        if image_to_show is None: return
        self.cv_image = image_to_show
        
        # Get the dimensions of the display frame
        frame_w, frame_h = self.image_frame.winfo_width(), self.image_frame.winfo_height()
        if frame_w <= 1 or frame_h <= 1: return # Avoid division by zero on startup
        img_h, img_w = image_to_show.shape[:2]
        
        # Calculate the scaling factor to fit the image
        scale = min(frame_w / img_w, frame_h / img_h)
        new_w, new_h = int(img_w * scale), int(img_h * scale)
        
        # Resize and convert color format (OpenCV: BGR -> PIL: RGB)
        display_image = cv2.resize(image_to_show, (new_w, new_h), interpolation=cv2.INTER_AREA)
        rgb_image = cv2.cvtColor(display_image, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_image)
        self.tk_image = ImageTk.PhotoImage(image=pil_image) # Must keep a reference
        self.image_label.config(image=self.tk_image)
        
    def _extract_entity_points(self, entity):
        """
        Extracts a list of (x, y) coordinate points from various DXF entity types.
        It handles simple entities like lines and complex ones like splines by
        "flattening" them into a series of straight line segments.
        """
        points = []
        dxf_type = entity.dxftype()
        try:
            if dxf_type == 'LINE':
                start, end = entity.dxf.start, entity.dxf.end
                points = [(start.x, start.y), (end.x, end.y)]
            elif dxf_type in ('LWPOLYLINE', 'POLYLINE'):
                if dxf_type == 'LWPOLYLINE':
                    # Modern polyline format
                    points = [(p[0], p[1]) for p in entity.get_points('xy')]
                else: 
                    # Older polyline format
                    points = [(p.dxf.location.x, p.dxf.location.y) for p in entity.vertices]
            elif dxf_type in ('CIRCLE', 'ARC', 'ELLIPSE', 'SPLINE'):
                # For curved entities, approximate them with a series of points.
                # `sagitta` controls the approximation accuracy.
                points = [(p.x, p.y) for p in entity.flattening(sagitta=0.01)]
        except Exception as e:
            # This can happen with complex or unsupported entity types
            print(f"                                 Warning: Could not process entity {dxf_type}: {e}")
        return points

    def _assemble_paths(self, segments):
        """
        Takes a list of line segments (start_point, end_point) and connects them
        end-to-end to form continuous polylines. This is crucial because DXF files
        often store complex shapes as many disconnected small lines or arcs.
        """
        if not segments: return []
        paths = []
        tolerance = 1e-6 # Tolerance for matching endpoints
        while segments:
            # Start a new path with the first available segment
            current_path = list(segments.pop(0))
            while True:
                extended = False
                # Search for another segment that connects to the end of our current path
                for i, segment in enumerate(segments):
                    p_start, p_end = segment
                    last_point_in_path = current_path[-1]
                    # Check if the start of the new segment matches the end of the path
                    if math.dist(last_point_in_path, p_start) < tolerance:
                        current_path.append(p_end)
                        segments.pop(i)
                        extended = True
                        break
                    # Check if the end of the new segment matches (and reverse it)
                    elif math.dist(last_point_in_path, p_end) < tolerance:
                        current_path.append(p_start)
                        segments.pop(i)
                        extended = True
                        break
                # If we couldn't find any connecting segments, this path is complete
                if not extended: break
            paths.append(current_path)
        return paths

    def reset_image_state(self):
        """Resets all state related to the loaded image."""
        self.image_path, self.cv_image, self.original_cv_image = None, None, None
        self.precomputed_mask = None
        self.lbl_image_status.config(text="Image: None")
        self.image_label.config(image='')
        self.last_transform_matrix = None
        self._check_files_loaded()

    def reset_dxf_state(self):
        """Resets all state related to the loaded DXF file."""
        self.dxf_path = None
        self.cad_features = {'outline': np.array([]), 'holes': [], 'creases': []}
        self.lbl_dxf_status.config(text="DXF: None")
        self.last_transform_matrix = None
        self._check_files_loaded()

    def align_with_moments_icp_pipeline(self):
        """
        A hybrid alignment strategy. First, it uses image moments for a fast, coarse
        alignment. Then, it refines this alignment using the Iterative Closest Point (ICP)
        algorithm. It tries two orientations (normal and Y-flipped) and picks the best one.
        This is robust to the CAD drawing's coordinate system origin.
        """
        print("1. Preprocessing: Extracting and preparing contours...")
        
        t_start_contour = time.time()
        img_contour = self.find_image_contour(self.original_cv_image)
        if self.debug_mode.get():
            elapsed_contour = time.time() - t_start_contour
            print(f"[DEBUG] T_ContourDetect (Outline): {elapsed_contour:.4f} seconds")

        if img_contour is None:
            print("  Error: No contour found in the image.")
            return None
        
        cad_contour_original = self.cad_features['outline'].reshape(-1, 2).astype(np.float32)
        
        # --- Pass 1: Try aligning with Y-Flipped CAD data ---
        # This handles cases where the CAD Y-axis points up, while the image Y-axis points down.
        print("\n--- Starting Pass 1: Y-Flipped (Image Coordinate System) ---")
        y_max = np.max(cad_contour_original[:, 1])
        cad_contour_yflipped = cad_contour_original.copy()
        cad_contour_yflipped[:, 1] = y_max - cad_contour_yflipped[:, 1]
        
        coarse_transform_1 = self._align_with_moments(cad_contour_yflipped, img_contour)
        if coarse_transform_1 is None:
                print("  Error: Coarse alignment failed on Pass 1.")
                return None
        
        final_transform_1, final_mse_1 = self._iterative_closest_point(source_points=cad_contour_yflipped, target_points=img_contour, initial_matrix=coarse_transform_1)
        if final_transform_1 is None:
            print("  Error: ICP failed on Pass 1.")
            return None
        print(f"  Pass 1 (Y-Flipped) Complete. MSE: {final_mse_1:.4f}")

        # --- Pass 2: Try aligning with original CAD data ---
        print("\n--- Starting Pass 2: Original (CAD Coordinate System) ---")
        coarse_transform_2 = self._align_with_moments(cad_contour_original, img_contour)
        if coarse_transform_2 is None:
            print("  Error: Coarse alignment failed on Pass 2.")
            return None

        final_transform_2, final_mse_2 = self._iterative_closest_point(source_points=cad_contour_original, target_points=img_contour, initial_matrix=coarse_transform_2)
        if final_transform_2 is None:
            print("  Error: ICP failed on Pass 2.")
            return None
        print(f"  Pass 2 (Original) Complete. MSE: {final_mse_2:.4f}")
        
        # --- Conclusion: Choose the best result ---
        # The alignment with the lower Mean Squared Error (MSE) is considered better.
        if final_mse_1 < final_mse_2:
            print(f"\n--- Y-Flipped alignment is better (MSE {final_mse_1:.4f} < {final_mse_2:.4f}). ---")
            # If the Y-flipped version was better, we need to build a final transform matrix
            # that includes the flip operation.
            y_flip_matrix = np.array([[1, 0, 0], [0, -1, y_max]], dtype=np.float32)
            T_flip_3x3 = np.vstack([y_flip_matrix, [0, 0, 1]])
            T_align_3x3 = np.vstack([final_transform_1, [0, 0, 1]])
            # The final transform is Align(Flip(Point))
            combined_transform_3x3 = T_align_3x3 @ T_flip_3x3
            return combined_transform_3x3[:2, :] # Return as a 2x3 matrix
        else:
            print(f"\n--- Original alignment is better (MSE {final_mse_2:.4f} <= {final_mse_1:.4f}). ---")
            return final_transform_2

    def align_with_homography_pipeline(self):
        """
        An advanced alignment pipeline that attempts to find a homography (perspective)
        transform. It first computes a baseline affine transform, then iteratively refines
        it to a homography by matching corners. If the homography doesn't improve the
        alignment error, it falls back to the more stable affine result.
        """
        print("--- Starting Smart Homography Pipeline (with MSE convergence) ---")
        # 1. Get a baseline affine transform to start with.
        print("1. Calculating baseline affine transformation...")
        M_affine = self.align_with_moments_icp_pipeline()
        if M_affine is None: return None, "Error"
        
        img_contour = self.find_image_contour(self.original_cv_image)
        if img_contour is None: return None, "Error"
        cad_contour = self.cad_features['outline'].reshape(-1, 2).astype(np.float32)
        
        # 2. Calculate the error of the baseline affine transform.
        affine_mse = self._calculate_alignment_error(cad_contour, img_contour, M_affine)
        if affine_mse is None: return None, "Error"
        print(f"  Benchmark Affine MSE: {affine_mse:.4f}")

        # 3. Extract keypoints (corners) from both CAD and image contours.
        print("4. Starting iterative homography refinement...")
        image_corners = self._extract_corners(img_contour, epsilon_factor=0.0015)
        cad_corners = self._extract_corners(cad_contour, epsilon_factor=0.0015)
        
        # Homography requires at least 4 matching point pairs.
        if image_corners is None or cad_corners is None or len(image_corners) < 4 or len(cad_corners) < 4:
            print("  Error: Insufficient corners for homography. Falling back to affine.")
            final_matrix = np.vstack([M_affine, [0, 0, 1]]) # Convert 2x3 to 3x3
            return final_matrix, "Affine Fallback (Not enough corners)"

        # 4. Iteratively refine the homography.
        current_H = np.vstack([M_affine, [0, 0, 1]]) # Start with the affine transform
        image_corner_kdtree = KDTree(image_corners) # For fast nearest-neighbor search
        previous_mse = affine_mse 
        
        for i in range(10): # Iterate a few times to converge
            # Transform CAD corners with the current homography and find the closest image corners
            cad_corners_transformed = cv2.perspectiveTransform(cad_corners.reshape(-1, 1, 2), current_H).reshape(-1, 2)
            _, indices = image_corner_kdtree.query(cad_corners_transformed)
            src_pts, dst_pts = cad_corners, image_corners[indices]
            
            # Find a new homography based on these refined correspondences
            next_H, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 20.0)
            if next_H is None: break
            
            # Check if the new homography actually improved the alignment
            current_mse = self._calculate_alignment_error(cad_contour, img_contour, next_H)
            if current_mse is None: break 
            
            # If the error didn't decrease significantly, we've converged.
            if current_mse >= previous_mse - 1e-6:
                break 
            
            current_H = next_H
            previous_mse = current_mse
        
        # 5. Final decision: use homography or fall back to affine.
        homography_mse = previous_mse
        print(f"  Final Homography MSE: {homography_mse:.4f}")
        if homography_mse < affine_mse:
            print("  SUCCESS: Homography improved alignment. Using perspective transform.")
            return current_H, "Iterative Homography"
        else:
            print("  INFO: Homography did not improve alignment. Falling back to affine transform.")
            final_matrix = np.vstack([M_affine, [0, 0, 1]])
            return final_matrix, "Affine Fallback (MSE did not improve)"

    def _calculate_alignment_error(self, source_contour, target_contour, transform_matrix):
        """
        Calculates the Mean Squared Error (MSE) between a transformed source contour
        and a target contour. A lower MSE indicates a better alignment. It uses a KD-Tree
        for efficient calculation of the distance from each point in the source to the
        nearest point in the target.
        """
        if source_contour is None or target_contour is None or transform_matrix is None: return None
        
        # Apply the transform (handles both 2x3 affine and 3x3 homography matrices)
        if transform_matrix.shape == (2, 3):
            transformed_source = cv2.transform(source_contour.reshape(-1, 1, 2), transform_matrix)
        elif transform_matrix.shape == (3, 3):
            transformed_source = cv2.perspectiveTransform(source_contour.reshape(-1, 1, 2), transform_matrix)
        else: return None
        
        transformed_source = transformed_source.reshape(-1, 2)
        target_contour_points = target_contour.reshape(-1, 2)
        try:
            kdtree = KDTree(target_contour_points)
            distances, _ = kdtree.query(transformed_source)
            return np.mean(np.square(distances)) # Return the mean of squared distances
        except Exception as e:
            print(f"  Error during MSE calculation: {e}")
            return None

    def _extract_corners(self, contour, epsilon_factor=0.001):
        """
        Approximates a contour with a simpler polygon using the Ramer-Douglas-Peucker
        algorithm. This is used to find the "corners" or key vertices of a shape.
        """
        if contour is None or len(contour) < 3: return None
        # Epsilon is the maximum distance a point can be from the simplified line to be considered part of it.
        epsilon = epsilon_factor * cv2.arcLength(contour, True)
        corners = cv2.approxPolyDP(contour, epsilon, True)
        return corners.reshape(-1, 2) if corners is not None else None

    def _align_with_moments(self, cad_contour, img_contour):
        """
        Performs a coarse alignment based on image moments. It calculates the centroid,
        scale, and rotation of both contours and computes the affine transformation
        required to match them.
        """
        try:
            M_cad = cv2.moments(cad_contour)
            M_img = cv2.moments(img_contour)
            if M_cad["m00"] == 0 or M_img["m00"] == 0: return None # Avoid division by zero
            
            # Calculate centroids (center of mass)
            cad_cx, cad_cy = M_cad["m10"] / M_cad["m00"], M_cad["m01"] / M_cad["m00"]
            img_cx, img_cy = M_img["m10"] / M_img["m00"], M_img["m01"] / M_img["m00"]
            
            # Estimate scale from the contour areas (m00)
            scale = math.sqrt(M_img["m00"] / M_cad["m00"])
            
            # Estimate rotation from the orientation of the principal axes
            get_orientation = lambda M: 0.5 * math.atan2(2 * M['mu11'], M['mu20'] - M['mu02'])
            angle_cad, angle_img = get_orientation(M_cad), get_orientation(M_img)
            rotation_angle_rad = angle_img - angle_cad
            
            # Build the 2x3 affine transformation matrix
            cos_a, sin_a = math.cos(rotation_angle_rad), math.sin(rotation_angle_rad)
            tx = img_cx - (scale * (cad_cx * cos_a - cad_cy * sin_a))
            ty = img_cy - (scale * (cad_cx * sin_a + cad_cy * cos_a))
            return np.array([[scale * cos_a, -scale * sin_a, tx], [scale * sin_a,  scale * cos_a, ty]], dtype=np.float32)
        except Exception as e:
            print(f"  Error during moments-based alignment: {e}")
            return None

    def _iterative_closest_point(self, source_points, target_points, initial_matrix, max_iterations=100, tolerance=1e-5):
        """
        Refines an initial alignment by iteratively matching points in the source cloud
        to the closest points in the target cloud and recalculating the transformation.
        """
        if initial_matrix is None: return None, float('inf')
        source_pts, target_pts = source_points.reshape(-1, 2), target_points.reshape(-1, 2)
        if len(source_pts) == 0 or len(target_pts) == 0: return None, float('inf')
        
        target_kdtree = KDTree(target_pts) # KDTree for fast point lookups
        current_transform = initial_matrix.copy()
        prev_error = float('inf')
        
        for i in range(max_iterations):
            # 1. Transform source points with the current matrix
            source_transformed = cv2.transform(source_pts.reshape(-1, 1, 2), current_transform).reshape(-1, 2)
            
            # 2. Find the closest target point for each transformed source point
            distances, indices = target_kdtree.query(source_transformed)
            correspondences = target_pts[indices]
            
            # 3. Estimate a new transformation based on these pairings
            if len(source_pts) < 3 or len(correspondences) < 3: return current_transform, prev_error
            new_transform, _ = cv2.estimateAffinePartial2D(source_pts, correspondences, method=cv2.LMEDS)
            if new_transform is None: return current_transform, prev_error
            
            # 4. Update and check for convergence
            current_transform = new_transform
            mean_error = np.mean(distances**2)
            if abs(prev_error - mean_error) < tolerance:
                return current_transform, mean_error # Converged
            prev_error = mean_error
            
        return current_transform, prev_error

    def run_visualization(self):
        """
        A debug function that displays the parsed CAD features and the detected
        image contour in separate windows for verification.
        """
        print("--- Running Input Visualization ---")
        if self.cad_features['outline'].size > 0:
            # Combine all CAD points to find the bounding box for proper scaling
            all_points_list = [self.cad_features['outline']] + self.cad_features['holes'] + self.cad_features['creases']
            all_points = np.vstack([p.reshape(-1, 2) for p in all_points_list if p.size > 0])
            if all_points.size == 0:
                messagebox.showerror("Visualize Error", "No points found in DXF features to visualize.")
                return
            
            x_min, y_min = np.min(all_points, axis=0)
            x_max, y_max = np.max(all_points, axis=0)
            
            # Create a black canvas to draw the CAD data onto
            CANVAS_W, CANVAS_H, padding = 800, 600, 50
            cad_w, cad_h = x_max - x_min, y_max - y_min
            if cad_w > 0 and cad_h > 0:
                scale = min((CANVAS_W - 2 * padding) / cad_w, (CANVAS_H - 2 * padding) / cad_h)
                cad_canvas = np.zeros((CANVAS_H, CANVAS_W, 3), dtype="uint8")
                
                # Function to transform CAD coordinates to canvas coordinates
                def transform_pt(points):
                    # Scale, flip Y-axis, and translate to fit canvas
                    return ((points - [x_min, y_min]) * [scale, -scale] + [padding, CANVAS_H - padding]).astype(np.int32)

                cv2.polylines(cad_canvas, [transform_pt(self.cad_features['outline'])], True, (255, 255, 255), 1)
                if self.cad_features['holes']:
                    cv2.polylines(cad_canvas, [transform_pt(p) for p in self.cad_features['holes']], True, (0, 255, 255), 1)
                if self.cad_features['creases']:
                    cv2.polylines(cad_canvas, [transform_pt(p) for p in self.cad_features['creases']], False, (255, 255, 0), 1)
                
                self._resize_for_display("DEBUG: Parsed CAD", cad_canvas)

        if self.original_cv_image is not None:
            image_contour = self.find_image_contour(self.original_cv_image)
            if image_contour is not None:
                debug_image = self.original_cv_image.copy()
                cv2.drawContours(debug_image, [image_contour.astype(np.int32)], -1, (255, 0, 255), 3)
                self._resize_for_display("DEBUG: Detected Image Contour", debug_image)
            else:
                messagebox.showinfo("Visualize", "No contour detected in the image.")
        messagebox.showinfo("Debug", "Debug windows opened.")

    def _resize_for_display(self, window_name, image, max_dim=900):
        """
        A utility function to resize an image for display in an OpenCV window,
        ensuring it doesn't exceed a maximum dimension.
        """
        h, w = image.shape[:2]
        if max_dim <= 0: max_dim=900
        if h > max_dim or w > max_dim:
            scale = max_dim / max(h, w)
            new_w, new_h = int(w * scale), int(h * scale)
            display_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
        else:
            display_image = image
        cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)
        cv2.imshow(window_name, display_image)
        cv2.waitKey(1) # Necessary to ensure the window updates

if __name__ == "__main__":
    # Standard Tkinter application startup
    root = tk.Tk()
    app = InspectionApp(root)
    root.mainloop()
